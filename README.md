# README

## 1. Environment Setup

To set up the environment, follow these steps:

```bash
conda env create -f environment.yml
conda activate MedAI
```

For OpenAI API Key, please save it in the .env file.

---

## 2. Data Description

The datasets used in this project can be downloaded from the following links:

- **MMLU** (Col_Med, Col_Bio, Pro_Med, Anatomy, Gene, Clinic): [MMLU Dataset](https://huggingface.co/datasets/cais/mmlu)
- **MedQA**: [MedQA Dataset](https://huggingface.co/datasets/GBaker/MedQA-USMLE-4-options)
- **MedMCQA**: [MedMCQA Dataset](https://huggingface.co/datasets/openlifescienceai/medmcqa)
- **PubMedQA**: [PubMedQA Dataset](https://huggingface.co/datasets/qiaojin/PubMedQA)

The data loading function is already implemented in the code.

The vector store is created using the [LangChain-FAISS](https://python.langchain.com/docs/integrations/vectorstores/faiss/) function. The source of the vector store is the 18 textbooks used for USMLE tests, released by MedQA: [Textbooks](https://github.com/jind11/MedQA). The chunking size is set to **1000**, and the overlap is **200**.

---

## 3. Running Baseline Models

The baseline models are implemented in the `methods.py`:

Run a baseline model with the following command:

```bash
python main.py --method zero_shot --dataset MMLU_Gene --model_name gpt-4o-mini --k=0
```

**Arguments:**
- `task`: `zero_shot`, `rag`, `rag_cot`, `rap`, `rat`
- `dataset`: `MMLU_Col_Med`, `MMLU_Col_Bio`, `MMLU_Pro_Med`, `MMLU_Anatomy`, `MMLU_Gene`, `MMLU_Clinic`,`medqa`, `medmcqa`, `pubmedqa`
- `model_name`: `gpt-4o-mini`, `gpt-3.5-turbo`, `Qwen/Qwen2.5-7B`, `Qwen/Qwen2.5-14B`, `Qwen/Qwen2.5-32B`, `Qwen/Qwen2.5-72B`, `LLaMA/llama-3-8B-Instruct`, `LLaMA/llama-3-70B-Instruct`
- `k`: The number of chunks to retrieve (default is 5)

---

## 4. QAG Generation

The QAG generation is also implemented in the `methods.py`:
- **Answer Generator**: `def run_qag_answer`
- **Question Speculator**: `def run_qag_speculator`

The **Question Speculator** and **Answer Generator** are implemented separately, as the generated speculative qa pairs can be used for different answer generator models.

Run the **Question Speculator** with:

```bash
python main.py --method qag_speculator --dataset MMLU_Gene --model_name gpt-4o-mini --k=5
```

**Arguments:**
- `task`: `qag_speculator`
- `dataset`: `medqa`, `medmcqa`, `pubmedqa`, `MMLU_Col_Med`, `MMLU_Col_Bio`, `MMLU_Pro_Med`, `MMLU_Anatomy`, `MMLU_Gene`, `MMLU_Clinic`
- `model_name`: `Qwen/Qwen2.5-7B`, `Qwen/Qwen2.5-14B`, `Qwen/Qwen2.5-32B`, `Qwen/Qwen2.5-72B`, `LLaMA/llama-3-8B-Instruct`, `LLaMA/llama-3-70B-Instruct`
- `k`: The number of chunks to retrieve

Run the **Answer Generator** with:

```bash
python main.py --method qag_answer --dataset MMLU_Gene --model_name gpt-4o-mini --k=5 --question_model 7B
```

**Arguments:**
- `task`: `qag_answer`
- `dataset`: `medqa`, `medmcqa`, `pubmedqa`, `MMLU_Col_Med`, `MMLU_Col_Bio`, `MMLU_Pro_Med`, `MMLU_Anatomy`, `MMLU_Gene`, `MMLU_Clinic`
- `model_name`: `Qwen/Qwen2.5-7B`, `Qwen/Qwen2.5-14B`, `Qwen/Qwen2.5-32B`, `Qwen/Qwen2.5-72B`, `LLaMA/llama-3-8B-Instruct`, `LLaMA/llama-3-70B-Instruct`
- `k`: The number of chunks to retrieve
- `question_model`: `7B`, `32B`, `72B` (choose the sub-questions and answers generated by which question speculator model)

---

## 5. Error Correction Analysis

run the `correction_analysis.py` with:

```bash
python correction_analysis.py
```

---
